{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 基本设置",
   "id": "606352003b7b38e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:09:41.319598Z",
     "start_time": "2025-04-30T10:09:40.137741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal, Dict\n",
    "from enum import Enum, IntEnum\n",
    "from dataclasses import dataclass\n",
    "from pprint import pprint"
   ],
   "id": "f300cac60ce781e8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:22:50.968510Z",
     "start_time": "2025-04-30T12:22:50.956502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class TinyLLaMAConfig:\n",
    "    '''https://github.com/jzhang38/TinyLlama?tab=readme-ov-file#training-details'''\n",
    "    '''https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/blob/main/config.json'''\n",
    "    # Transformer block 的数量\n",
    "    num_decoder_blocks: int = 22\n",
    "\n",
    "    # 模型最大可处理的上下文长度\n",
    "    context_length: int = 2048\n",
    "\n",
    "    # token 向量的维度\n",
    "    n_embd: int = 2048\n",
    "\n",
    "    # 前馈网络（MLP）中间层的维度\n",
    "    ffw_size: int = 5632\n",
    "\n",
    "    # 多头注意力的头数（head 数量）\n",
    "    n_head: int = 32\n",
    "\n",
    "    # 模型支持的 token 数量（即词表大小）\n",
    "    vocab_size: int = 32000\n",
    "\n",
    "    # 是否为 Linear 层添加 bias（偏置项）\n",
    "    bias: Literal[False] = False\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert self.ffw_size >= self.n_embd, \"ffw_size should be ≥ n_embd\"\n",
    "        assert self.bias is False, \"bias must be False in this experiment.\"\n",
    "\n",
    "\n",
    "class TinyLLaMAModelType(Enum):\n",
    "    # TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "    TINY_LLAMA_1B = \"/nfs/home/xiaoxiao/models/hf_models/TinyLlama-1.1B\"\n",
    "\n",
    "\n",
    "class ByteUnits(IntEnum):\n",
    "    B = 1  # Byte = 1 byte\n",
    "    KB = 1000  # Kilobyte = 10^3 bytes\n",
    "    MB = 1000 ** 2  # Megabyte = 10^6 bytes\n",
    "    GB = 1000 ** 3  # Gigabyte = 10^9 bytes\n",
    "\n",
    "\n",
    "class FloatingPointPrecision(IntEnum):\n",
    "    FP32 = 4  # 32-bit floating-point, 4 bytes\n",
    "    FP16 = 2  # 16-bit floating-point, 2 bytes\n",
    "    BFLOAT16 = 2  # bfloat16, 16-bit, 2 bytes\n",
    "\n",
    "\n",
    "class GPUMemory(Enum):\n",
    "    A100_40GB = 40e9  # 40 GB for NVIDIA A100\n",
    "    V100_16GB = 16e9  # 16 GB for NVIDIA V100\n",
    "    V100_32GB = 32e9  # 32 GB for NVIDIA V100\n",
    "    T4_16GB = 16e9  # 16 GB for NVIDIA T4\n",
    "    P100_16GB = 16e9  # 16 GB for NVIDIA P100\n",
    "    RTX4090_24GB = 24e9  # 24 GB for NVIDIA RTX 4090\n",
    "\n",
    "\n",
    "class GPU:\n",
    "    def __init__(self, name: str, flops: Dict[FloatingPointPrecision, float]) -> None:\n",
    "        self.name = name\n",
    "        self.flops = flops\n",
    "\n",
    "\n",
    "class RTX4090(GPU):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"RTX 4090\", {\n",
    "            FloatingPointPrecision.FP32: 82.6e12,  # 82.6 TFLOPs\n",
    "            FloatingPointPrecision.FP16: 165.2e12,  # 165.2 TFLOPs\n",
    "            FloatingPointPrecision.BFLOAT16: 165.2e12\n",
    "        })"
   ],
   "id": "c6899bf8488b7289",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:08:48.245702Z",
     "start_time": "2025-04-30T10:08:48.241078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tinyllama_config = TinyLLaMAConfig()\n",
    "pprint(tinyllama_config)"
   ],
   "id": "d1d83de80e24387f",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLLaMAConfig(num_decoder_blocks=22,\n",
      "                context_length=2048,\n",
      "                n_embd=2048,\n",
      "                ffw_size=5632,\n",
      "                n_head=32,\n",
      "                vocab_size=32000,\n",
      "                bias=False)\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 可训练参数总数",
   "id": "3d0962bebfb5559e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:18:46.808860Z",
     "start_time": "2025-04-30T10:18:46.419179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ],
   "id": "5f7c0d0f5def1083",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T10:59:08.232843Z",
     "start_time": "2025-04-30T10:59:08.228590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def total_trainable_parameters(model: torch.nn.Module) -> int:\n",
    "    \"\"\"Returns the number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())"
   ],
   "id": "c28f6f0edf557c42",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:21:02.428193Z",
     "start_time": "2025-04-30T12:21:01.894732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tinyllama = LlamaForCausalLM.from_pretrained(TinyLLaMAModelType.TINY_LLAMA_1B.value)\n",
    "\n",
    "tinyllama_params = total_trainable_parameters(tinyllama)\n",
    "\n",
    "print(\n",
    "    f\"Number of trainable parameters in TinyLlama model: {tinyllama_params_no_bias}.\\n\"\n",
    ")\n",
    "\n",
    "print(tinyllama)"
   ],
   "id": "4cf1791c7afe17b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in TinyLlama model: 1100048384.\n",
      "\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:20:05.392102Z",
     "start_time": "2025-04-30T12:20:05.387098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def params(\n",
    "        num_decoder_blocks: int,\n",
    "        context_length: int,\n",
    "        n_embd: int,\n",
    "        ffw_size: int,\n",
    "        vocab_size: int,\n",
    "        n_head: int,\n",
    "        n_kv_head: int,\n",
    ") -> OrderedDict[str, int]:\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token embeddings\n",
    "    out[\"embedding\"] = vocab_size * n_embd\n",
    "\n",
    "    # attention blocks\n",
    "    # Grouped Query Attention\n",
    "    head_dim = n_embd // n_head\n",
    "    kv_dim = n_kv_head * head_dim\n",
    "\n",
    "    out[\"attention/q_proj\"] = n_embd * n_embd\n",
    "    out[\"attention/k_proj\"] = n_embd * kv_dim\n",
    "    out[\"attention/v_proj\"] = n_embd * kv_dim\n",
    "    out[\"attention/o_proj\"] = n_embd * n_embd\n",
    "    out[\"attention\"] = (\n",
    "            out[\"attention/q_proj\"]\n",
    "            + out[\"attention/k_proj\"]\n",
    "            + out[\"attention/v_proj\"]\n",
    "            + out[\"attention/o_proj\"]\n",
    "    )\n",
    "\n",
    "    # MLP blocks\n",
    "    out[\"mlp/gate_proj\"] = n_embd * ffw_size\n",
    "    out[\"mlp/up_proj\"] = n_embd * ffw_size\n",
    "    out[\"mlp/down_proj\"] = ffw_size * n_embd\n",
    "    out[\"mlp\"] = (\n",
    "            out[\"mlp/gate_proj\"]\n",
    "            + out[\"mlp/up_proj\"]\n",
    "            + out[\"mlp/down_proj\"]\n",
    "    )\n",
    "\n",
    "    # RMS Norm layers\n",
    "    out[\"rms/input_layernorm\"] = n_embd\n",
    "    out[\"rms/post_attention_layernorm\"] = n_embd\n",
    "    out[\"rms\"] = out[\"rms/input_layernorm\"] + out[\"rms/post_attention_layernorm\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"] + out[\"rms\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "\n",
    "    # Final Norm\n",
    "    out[\"final_norm\"] = n_embd\n",
    "\n",
    "    # LM Head\n",
    "    out[\"lm_head\"] = vocab_size * n_embd\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = out[\"embedding\"] + out[\"transformer\"] + out[\"lm_head\"] + out[\"final_norm\"]\n",
    "\n",
    "    return out"
   ],
   "id": "aaaf5fe44d94c329",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:20:40.129555Z",
     "start_time": "2025-04-30T12:20:40.120044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params_dict = params(num_decoder_blocks=TinyLLaMAConfig.num_decoder_blocks,\n",
    "                     context_length=TinyLLaMAConfig.context_length, n_embd=TinyLLaMAConfig.n_embd,\n",
    "                     ffw_size=TinyLLaMAConfig.ffw_size, vocab_size=TinyLLaMAConfig.vocab_size,\n",
    "                     n_head=TinyLLaMAConfig.n_head, n_kv_head=4)\n",
    "tinyllama_params_no_bias_manual = params_dict[\"total\"]\n",
    "\n",
    "# Compare to expected PyTorch model parameter count\n",
    "expected_params = tinyllama_params_no_bias\n",
    "comparison_result = tinyllama_params_no_bias_manual == expected_params\n",
    "comparison_msg = f\"We see: {tinyllama_params_no_bias_manual}, Expected: {expected_params}, Match: {comparison_result}\"\n",
    "\n",
    "data = {\n",
    "    \"Name\": params_dict.keys(),\n",
    "    \"Parameters\": params_dict.values(),\n",
    "    \"Ratio (%)\": [value / tinyllama_params_no_bias_manual * 100 for value in params_dict.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Printing comparison result and parameter distribution table\n",
    "print(comparison_msg + \"\\n\")\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))"
   ],
   "id": "8c8ee82745c11975",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see: 1100048384, Expected: 1100048384, Match: True\n",
      "\n",
      "+------------------------------+------------+------------------------+\n",
      "|             Name             | Parameters |       Ratio (%)        |\n",
      "+------------------------------+------------+------------------------+\n",
      "|          embedding           |  65536000  |    5.95755613600356    |\n",
      "|       attention/q_proj       |  4194304   |   0.3812835927042278   |\n",
      "|       attention/k_proj       |   524288   |  0.047660449088028474  |\n",
      "|       attention/v_proj       |   524288   |  0.047660449088028474  |\n",
      "|       attention/o_proj       |  4194304   |   0.3812835927042278   |\n",
      "|          attention           |  9437184   |   0.8578880835845126   |\n",
      "|        mlp/gate_proj         |  11534336  |   1.0485298799366265   |\n",
      "|         mlp/up_proj          |  11534336  |   1.0485298799366265   |\n",
      "|        mlp/down_proj         |  11534336  |   1.0485298799366265   |\n",
      "|             mlp              |  34603008  |   3.1455896398098795   |\n",
      "|     rms/input_layernorm      |    2048    | 0.00018617362925011123 |\n",
      "| rms/post_attention_layernorm |    2048    | 0.00018617362925011123 |\n",
      "|             rms              |    4096    | 0.00037234725850022245 |\n",
      "|            block             |  44044288  |   4.003850070652892    |\n",
      "|         transformer          | 968974336  |   88.08470155436363    |\n",
      "|          final_norm          |    2048    | 0.00018617362925011123 |\n",
      "|           lm_head            |  65536000  |    5.95755613600356    |\n",
      "|            total             | 1100048384 |         100.0          |\n",
      "+------------------------------+------------+------------------------+\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 计算 Checkpoint 大小和 Fluff Ratio",
   "id": "dc6cfedb95ebf5f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from safetensors import safe_open",
   "id": "107f3501cf2c616"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:43:50.950711Z",
     "start_time": "2025-04-30T12:43:50.944064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_checkpoint_size(params_count: int, precision: FloatingPointPrecision, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the estimated checkpoint size in specified units.\n",
    "\n",
    "    This function estimates the checkpoint size for a model given the number\n",
    "    of parameters, the precision of these parameters, and\n",
    "    the desired units for the result. It accounts for the AdamW optimizer's\n",
    "    storage requirements by adding two times the parameter bytes to account\n",
    "    for the optimizer's moment and velocity vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params_count : int\n",
    "        The number of parameters excluding biases.\n",
    "    precision : FloatingPointPrecision\n",
    "        The floating point precision of the parameters.\n",
    "    units : ByteUnits\n",
    "        The units for the resulting checkpoint size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The estimated checkpoint size in the specified units.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The AdamW optimizer requires additional storage for each parameter\n",
    "    for maintaining momentum and variance vectors, hence the calculation\n",
    "    includes 2 * params_bytes to accommodate these.\n",
    "    \"\"\"\n",
    "    params_bytes = params_count * precision.value\n",
    "    params_and_buffers_bytes = params_bytes\n",
    "    return params_and_buffers_bytes / units.value\n",
    "\n",
    "\n",
    "def calculate_fluff_ratio(measured_bytes: int, estimated_bytes: float, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the fluff ratio between measured and estimated checkpoint sizes.\n",
    "\n",
    "    The fluff ratio is a measure of the overhead or additional data in the\n",
    "    checkpoint file, expressed as a percentage of the estimated size. This\n",
    "    function converts the estimated size from gigabytes (or specified units)\n",
    "    to bytes before calculating the ratio to ensure consistency in units.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    measured_bytes : int\n",
    "        The actual size of the checkpoint file, in bytes.\n",
    "    estimated_bytes : float\n",
    "        The estimated size of the checkpoint file, in the specified units.\n",
    "    units : ByteUnits\n",
    "        The units in which the estimated bytes are provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The fluff ratio, expressed as a percentage.\n",
    "    \"\"\"\n",
    "    estimated_bytes_in_bytes = estimated_bytes * units.value\n",
    "    return (measured_bytes / estimated_bytes_in_bytes) * 100"
   ],
   "id": "c7e7d9ad7bfc34da",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:45:25.216398Z",
     "start_time": "2025-04-30T12:45:25.208849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tinyllama_checkpoint_size_measured_in_bytes = 2200119864  # from 'wc -c model.safetensors'\n",
    "tinyllama_checkpoint_size_measured_in_gb = tinyllama_checkpoint_size_measured_in_bytes / ByteUnits.GB\n",
    "\n",
    "tinyllama_checkpoint_size_estimated_in_bytes = calculate_checkpoint_size(\n",
    "    params_count=tinyllama_params,\n",
    "    precision=FloatingPointPrecision.BFLOAT16,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "tinyllama_checkpoint_size_estimated_in_gb = tinyllama_checkpoint_size_estimated_in_bytes / ByteUnits.GB\n",
    "\n",
    "fluff_ratio = calculate_fluff_ratio(\n",
    "    measured_bytes=tinyllama_checkpoint_size_measured_in_bytes,\n",
    "    estimated_bytes=tinyllama_checkpoint_size_estimated_in_bytes,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "\n",
    "data = [\n",
    "    [\"Measured Checkpoint Size (bytes)\", tinyllama_checkpoint_size_measured_in_bytes],\n",
    "    [\"Measured Checkpoint Size (GB)\", tinyllama_checkpoint_size_measured_in_gb],\n",
    "    [\"Estimated Checkpoint Size (bytes)\", tinyllama_checkpoint_size_estimated_in_bytes],\n",
    "    [\"Estimated Checkpoint Size (GB)\", tinyllama_checkpoint_size_estimated_in_gb],\n",
    "    [\"Fluff Ratio\", fluff_ratio],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))"
   ],
   "id": "c54033d8e9354c82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------------------+\n",
      "|              Metric               |       Value        |\n",
      "+-----------------------------------+--------------------+\n",
      "| Measured Checkpoint Size (bytes)  |     2200119864     |\n",
      "|   Measured Checkpoint Size (GB)   |    2.200119864     |\n",
      "| Estimated Checkpoint Size (bytes) |    2200096768.0    |\n",
      "|  Estimated Checkpoint Size (GB)   |    2.200096768     |\n",
      "|            Fluff Ratio            | 100.00104977200712 |\n",
      "+-----------------------------------+--------------------+\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPU Memory Footprint",
   "id": "8a69ab197fcd9429"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:44:14.772976Z",
     "start_time": "2025-04-30T12:44:14.767695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_memory_ratio(checkpoint_size: float, gpu_memory: GPUMemory) -> str:\n",
    "    memory_ratio = checkpoint_size / gpu_memory.value * 100\n",
    "    return f\"Memory ratio taken up just for parameters: {memory_ratio:.2f}%\"\n",
    "\n",
    "\n",
    "print(calculate_memory_ratio(checkpoint_size=tinyllama_checkpoint_size_estimated_in_bytes,\n",
    "                             gpu_memory=GPUMemory.RTX4090_24GB))"
   ],
   "id": "f69b7e8d6589f982",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory ratio taken up just for parameters: 9.17%\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
