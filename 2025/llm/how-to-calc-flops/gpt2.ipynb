{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configurations, Constants and Enums",
   "id": "1dca82d434f34676"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:37:24.121507Z",
     "start_time": "2025-04-30T08:37:24.108692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal, Dict\n",
    "from enum import Enum, IntEnum\n",
    "from dataclasses import dataclass\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    num_decoder_blocks: int = 12\n",
    "    context_length: int = 1024\n",
    "    n_embd: int = 768\n",
    "    ffw_size: int = 3072  # note, this is 4 * n_embd\n",
    "    n_head: int = 12\n",
    "    vocab_size: int = 50257\n",
    "    bias: Literal[False] = False\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert self.ffw_size == 4 * self.n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "        assert self.bias is False, \"bias must be False in this experiment.\"\n",
    "\n",
    "\n",
    "class GPT2ModelType(Enum):\n",
    "    GPT2 = \"gpt2\"\n",
    "    GPT2_MEDIUM = \"gpt2-medium\"\n",
    "    GPT2_LARGE = \"gpt2-large\"\n",
    "    GPT2_XL = \"gpt2-xl\"\n",
    "\n",
    "\n",
    "class ByteUnits(IntEnum):\n",
    "    B = 1  # Byte = 1 byte\n",
    "    KB = 1000  # Kilobyte = 10^3 bytes\n",
    "    MB = 1000 ** 2  # Megabyte = 10^6 bytes\n",
    "    GB = 1000 ** 3  # Gigabyte = 10^9 bytes\n",
    "\n",
    "\n",
    "class FloatingPointPrecision(IntEnum):\n",
    "    FP32 = 4  # 32-bit floating-point, 4 bytes\n",
    "    FP16 = 2  # 16-bit floating-point, 2 bytes\n",
    "    BFLOAT16 = 2  # bfloat16, 16-bit, 2 bytes\n",
    "\n",
    "\n",
    "class GPUMemory(Enum):\n",
    "    A100_40GB = 40e9  # 40 GB for NVIDIA A100\n",
    "    V100_16GB = 16e9  # 16 GB for NVIDIA V100\n",
    "    V100_32GB = 32e9  # 32 GB for NVIDIA V100\n",
    "    T4_16GB = 16e9  # 16 GB for NVIDIA T4\n",
    "    P100_16GB = 16e9  # 16 GB for NVIDIA P100\n",
    "    RTX4090_24GB = 24e9  # 24 GB for NVIDIA RTX 4090\n",
    "\n",
    "\n",
    "class GPU:\n",
    "    def __init__(self, name: str, flops: Dict[FloatingPointPrecision, float]) -> None:\n",
    "        self.name = name\n",
    "        self.flops = flops\n",
    "\n",
    "\n",
    "class A100(GPU):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"A100\", {\n",
    "            FloatingPointPrecision.FP32: 19.5e12,\n",
    "            FloatingPointPrecision.FP16: 312e12,\n",
    "            FloatingPointPrecision.BFLOAT16: 312e12\n",
    "        })\n",
    "\n",
    "\n",
    "class RTX4090(GPU):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"RTX 4090\", {\n",
    "            FloatingPointPrecision.FP32: 82.6e12,\n",
    "            FloatingPointPrecision.FP16: 165.2e12,\n",
    "            FloatingPointPrecision.BFLOAT16: 165.2e12\n",
    "        })"
   ],
   "id": "a9c48b41051b95ae",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:21:35.991554Z",
     "start_time": "2025-04-30T08:21:35.987463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpt2_config = GPTConfig()\n",
    "pprint(gpt2_config)"
   ],
   "id": "bb373a3408cde101",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTConfig(num_decoder_blocks=12,\n",
      "          context_length=1024,\n",
      "          n_embd=768,\n",
      "          ffw_size=3072,\n",
      "          n_head=12,\n",
      "          vocab_size=50257,\n",
      "          bias=False)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Total Trainable Parameters",
   "id": "b0e1253f6349490f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:27:22.558482Z",
     "start_time": "2025-04-30T08:27:22.555405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ],
   "id": "f173932bca79064a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:23:01.765824Z",
     "start_time": "2025-04-30T08:23:01.762132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def total_trainable_parameters(model: torch.nn.Module, include_bias: bool = True) -> int:\n",
    "    \"\"\"Returns the number of trainable parameters in the model.\"\"\"\n",
    "    if not include_bias:\n",
    "        return sum(p.numel() for name, p in model.named_parameters() if p.requires_grad and \"bias\" not in name)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "id": "d65a8478d392d41",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:34:35.412506Z",
     "start_time": "2025-04-30T08:34:35.312363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = '/nfs/home/xiaoxiao/models/hf_models/gpt2'\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "gpt2_params_no_bias = total_trainable_parameters(gpt2, include_bias=False)\n",
    "gpt2_params_with_bias = total_trainable_parameters(gpt2, include_bias=True)\n",
    "\n",
    "print(\n",
    "    f\"Number of trainable parameters in GPT2 model: {gpt2_params_no_bias} (excluding bias) and {gpt2_params_with_bias} (including bias).\"\n",
    ")"
   ],
   "id": "a4cd029d7ead55c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in GPT2 model: 124337664 (excluding bias) and 124439808 (including bias).\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:24:49.244175Z",
     "start_time": "2025-04-30T08:24:49.235577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def params(\n",
    "        num_decoder_blocks: int = 12,\n",
    "        context_length: int = 1024,\n",
    "        n_embd: int = 768,\n",
    "        ffw_size: int = 3072,\n",
    "        vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = n_embd * context_length\n",
    "    out[\"embedding/token\"] = n_embd * vocab_size\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = n_embd  # note, bias=False in our LN\n",
    "    out[\"attention/kqv\"] = n_embd * 3 * n_embd\n",
    "    out[\"attention/proj\"] = n_embd ** 2\n",
    "    out[\"attention\"] = out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "\n",
    "    # MLP blocks\n",
    "    assert ffw_size == 4 * n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "    out[\"mlp/ln\"] = n_embd\n",
    "    out[\"mlp/ffw\"] = n_embd * ffw_size\n",
    "    out[\"mlp/proj\"] = ffw_size * n_embd\n",
    "    out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"ln_f\"] = n_embd  # final layernorm\n",
    "    out[\"dense\"] = 0  # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"dense\"]\n",
    "\n",
    "    return out"
   ],
   "id": "7affe40b481e52b9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:34:40.535909Z",
     "start_time": "2025-04-30T08:34:40.530269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params_dict = params()\n",
    "gpt2_params_no_bias_manual = params_dict[\"total\"]\n",
    "\n",
    "# Compare to expected PyTorch model parameter count\n",
    "expected_params = gpt2_params_no_bias\n",
    "comparison_result = gpt2_params_no_bias_manual == expected_params\n",
    "comparison_msg = f\"We see: {gpt2_params_no_bias_manual}, Expected: {expected_params}, Match: {comparison_result}\"\n",
    "\n",
    "data = {\n",
    "    \"Name\": params_dict.keys(),\n",
    "    \"Parameters\": params_dict.values(),\n",
    "    \"Ratio (%)\": [value / gpt2_params_no_bias_manual * 100 for value in params_dict.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Printing comparison result and parameter distribution table\n",
    "print(comparison_msg + \"\\n\")\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))"
   ],
   "id": "57b44c80870abf72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see: 124337664, Expected: 124337664, Match: True\n",
      "\n",
      "+--------------------+------------+-----------------------+\n",
      "|        Name        | Parameters |       Ratio (%)       |\n",
      "+--------------------+------------+-----------------------+\n",
      "| embedding/position |   786432   |  0.6324970042866496   |\n",
      "|  embedding/token   |  38597376  |  31.042384711361475   |\n",
      "|     embedding      |  39383808  |  31.674881715648123   |\n",
      "|    attention/ln    |    768     | 0.0006176728557486812 |\n",
      "|   attention/kqv    |  1769472   |  1.4231182596449616   |\n",
      "|   attention/proj   |   589824   |  0.47437275321498723  |\n",
      "|     attention      |  2360064   |  1.8981086857156975   |\n",
      "|       mlp/ln       |    768     | 0.0006176728557486812 |\n",
      "|      mlp/ffw       |  2359296   |   1.897491012859949   |\n",
      "|      mlp/proj      |  2359296   |   1.897491012859949   |\n",
      "|        mlp         |  4719360   |   3.795599698575646   |\n",
      "|       block        |  7079424   |   5.693708384291344   |\n",
      "|    transformer     |  84953088  |   68.32450061149613   |\n",
      "|        ln_f        |    768     | 0.0006176728557486812 |\n",
      "|       dense        |     0      |          0.0          |\n",
      "|       total        | 124337664  |         100.0         |\n",
      "+--------------------+------------+-----------------------+\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculating Checkpoint Size and Fluff Ratio",
   "id": "9683c06e4f18f8d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:34:43.030235Z",
     "start_time": "2025-04-30T08:34:43.023475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_checkpoint_size(params_count: int, precision: FloatingPointPrecision, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the estimated checkpoint size in specified units.\n",
    "\n",
    "    This function estimates the checkpoint size for a model given the number\n",
    "    of parameters, the precision of these parameters, and\n",
    "    the desired units for the result. It accounts for the AdamW optimizer's\n",
    "    storage requirements by adding two times the parameter bytes to account\n",
    "    for the optimizer's moment and velocity vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params_count : int\n",
    "        The number of parameters excluding biases.\n",
    "    precision : FloatingPointPrecision\n",
    "        The floating point precision of the parameters.\n",
    "    units : ByteUnits\n",
    "        The units for the resulting checkpoint size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The estimated checkpoint size in the specified units.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The AdamW optimizer requires additional storage for each parameter\n",
    "    for maintaining momentum and variance vectors, hence the calculation\n",
    "    includes 2 * params_bytes to accommodate these.\n",
    "    \"\"\"\n",
    "    params_bytes = params_count * precision.value\n",
    "    params_and_buffers_bytes = params_bytes + 2 * params_bytes  # AdamW optimizer buffers\n",
    "    return params_and_buffers_bytes / units.value\n",
    "\n",
    "\n",
    "def calculate_fluff_ratio(measured_bytes: int, estimated_bytes: float, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the fluff ratio between measured and estimated checkpoint sizes.\n",
    "\n",
    "    The fluff ratio is a measure of the overhead or additional data in the\n",
    "    checkpoint file, expressed as a percentage of the estimated size. This\n",
    "    function converts the estimated size from gigabytes (or specified units)\n",
    "    to bytes before calculating the ratio to ensure consistency in units.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    measured_bytes : int\n",
    "        The actual size of the checkpoint file, in bytes.\n",
    "    estimated_bytes : float\n",
    "        The estimated size of the checkpoint file, in the specified units.\n",
    "    units : ByteUnits\n",
    "        The units in which the estimated bytes are provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The fluff ratio, expressed as a percentage.\n",
    "    \"\"\"\n",
    "    estimated_bytes_in_bytes = estimated_bytes * units.value\n",
    "    return (measured_bytes / estimated_bytes_in_bytes) * 100"
   ],
   "id": "c7330f219ecbae75",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:44:25.285530Z",
     "start_time": "2025-04-30T09:44:25.278472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpt2_checkpoint_size_measured_in_bytes = 1542470366  # from 'wc -c ckpt.pt'\n",
    "gpt2_checkpoint_size_measured_in_gb = gpt2_checkpoint_size_measured_in_bytes / ByteUnits.GB\n",
    "\n",
    "gpt2_checkpoint_size_estimated_in_bytes = calculate_checkpoint_size(\n",
    "    params_count=gpt2_params_no_bias,\n",
    "    precision=FloatingPointPrecision.FP32,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "gpt2_checkpoint_size_estimated_in_gb = gpt2_checkpoint_size_estimated_in_bytes / ByteUnits.GB\n",
    "\n",
    "fluff_ratio = calculate_fluff_ratio(\n",
    "    measured_bytes=gpt2_checkpoint_size_measured_in_bytes,\n",
    "    estimated_bytes=gpt2_checkpoint_size_estimated_in_bytes,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "\n",
    "data = [\n",
    "    [\"Measured Checkpoint Size (bytes)\", gpt2_checkpoint_size_measured_in_bytes],\n",
    "    [\"Measured Checkpoint Size (GB)\", gpt2_checkpoint_size_measured_in_gb],\n",
    "    [\"Estimated Checkpoint Size (bytes)\", gpt2_checkpoint_size_estimated_in_bytes],\n",
    "    [\"Estimated Checkpoint Size (GB)\", gpt2_checkpoint_size_estimated_in_gb],\n",
    "    [\"Fluff Ratio\", fluff_ratio],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))"
   ],
   "id": "5750dcf7533c9a1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------------+\n",
      "|              Metric               |       Value       |\n",
      "+-----------------------------------+-------------------+\n",
      "| Measured Checkpoint Size (bytes)  |    1542470366     |\n",
      "|   Measured Checkpoint Size (GB)   |    1.542470366    |\n",
      "| Estimated Checkpoint Size (bytes) |   1492051968.0    |\n",
      "|  Estimated Checkpoint Size (GB)   |    1.492051968    |\n",
      "|            Fluff Ratio            | 103.3791314968461 |\n",
      "+-----------------------------------+-------------------+\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPU Memory Footprint of Loading Model and Optimizer",
   "id": "2573740859efa6bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:38:21.055701Z",
     "start_time": "2025-04-30T08:38:21.050226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_memory_ratio(checkpoint_size: float, gpu_memory: GPUMemory) -> str:\n",
    "    memory_ratio = checkpoint_size / gpu_memory.value * 100\n",
    "    return f\"Memory ratio taken up just for parameters: {memory_ratio:.2f}%\"\n",
    "\n",
    "\n",
    "print(\n",
    "    calculate_memory_ratio(checkpoint_size=gpt2_checkpoint_size_estimated_in_bytes, gpu_memory=GPUMemory.RTX4090_24GB))"
   ],
   "id": "61b9250bdc221f90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory ratio taken up just for parameters: 6.22%\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Estimating FLOPs for a Single Forward Pass",
   "id": "6a937cc28bc24e22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:39:22.827057Z",
     "start_time": "2025-04-30T08:39:22.817783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flops(\n",
    "        num_decoder_blocks: int = 12,\n",
    "        context_length: int = 1024,\n",
    "        n_embd: int = 768,\n",
    "        n_head: int = 12,\n",
    "        ffw_size: int = 3072,\n",
    "        vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    # we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = n_embd // n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/kqv\"] = 2 * context_length * (n_embd * 3 * n_embd)\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * context_length * context_length * n_embd\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * n_head * (context_length * context_length * head_size)\n",
    "    # 4) the final linear projection\n",
    "    out[\"attention/proj\"] = 2 * context_length * (n_embd * n_embd)\n",
    "    out[\"attention\"] = sum(out[\"attention/\" + k] for k in [\"kqv\", \"scores\", \"reduce\", \"proj\"])\n",
    "\n",
    "    # MLP blocks\n",
    "    ffw_size = 4 * n_embd  # feed forward size\n",
    "    out[\"mlp/ffw1\"] = 2 * context_length * (n_embd * ffw_size)\n",
    "    out[\"mlp/ffw2\"] = 2 * context_length * (ffw_size * n_embd)\n",
    "    out[\"mlp\"] = out[\"mlp/ffw1\"] + out[\"mlp/ffw2\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"dense\"] = 2 * context_length * (n_embd * vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"dense\"]\n",
    "    out[\"backward_total\"] = 2 * out[\"forward_total\"]  # use common estimate of bwd = 2*fwd\n",
    "    out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    return out"
   ],
   "id": "ecfcd7e9755191ba",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:39:45.263664Z",
     "start_time": "2025-04-30T08:39:45.257258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f = flops()\n",
    "flops_total = f[\"forward_total\"]\n",
    "\n",
    "table = [(\"name\", \"flops\", \"ratio (%)\")]\n",
    "for k, v in f.items():\n",
    "    table.append((k, v, v / flops_total * 100))\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"pretty\", numalign=\"right\"))"
   ],
   "id": "cfa52a0c038009d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+---------------------+\n",
      "|       name       |    flops     |      ratio (%)      |\n",
      "+------------------+--------------+---------------------+\n",
      "|  attention/kqv   |  3623878656  | 1.2425508965889174  |\n",
      "| attention/scores |  1610612736  | 0.5522448429284077  |\n",
      "| attention/reduce |  1610612736  | 0.5522448429284077  |\n",
      "|  attention/proj  |  1207959552  | 0.41418363219630583 |\n",
      "|    attention     |  8053063680  | 2.7612242146420387  |\n",
      "|     mlp/ffw1     |  4831838208  | 1.6567345287852233  |\n",
      "|     mlp/ffw2     |  4831838208  | 1.6567345287852233  |\n",
      "|       mlp        |  9663676416  | 3.3134690575704466  |\n",
      "|      block       | 17716740096  |  6.074693272212485  |\n",
      "|   transformer    | 212600881152 |  72.89631926654981  |\n",
      "|      dense       | 79047426048  |  27.10368073345018  |\n",
      "|  forward_total   | 291648307200 |        100.0        |\n",
      "|  backward_total  | 583296614400 |        200.0        |\n",
      "|      total       | 874944921600 |        300.0        |\n",
      "+------------------+--------------+---------------------+\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model FLOPs Utilization (MFU)",
   "id": "5fa4c5e824f19569"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:41:05.911954Z",
     "start_time": "2025-04-30T08:41:05.906961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# here is what we currently roughly measure\n",
    "batch_size = 20 * 5  # 5 is grad_accum, so total batch size is 100\n",
    "measured_time = 0.755  # in seconds per iteration\n",
    "measured_throughput = batch_size / measured_time # number of samples processed per second\n",
    "flops_achieved_per_second = f[\"total\"] * measured_throughput\n",
    "\n",
    "# A100 is cited to be 312 TFLOPS of bfloat16 running on tensor cores\n",
    "a100_bfloat16_promised_flops = 312e12\n",
    "\n",
    "# the fraction of the A100 that we are using:\n",
    "print(f\"fraction of A100 used: {flops_achieved_per_second / a100_bfloat16_promised_flops * 100:.2f}%\")"
   ],
   "id": "ad7d58affa87344d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of A100 used: 37.14%\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Theoretical FLOPs in Transformer Models",
   "id": "c2338149def8bbc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T08:45:53.629098Z",
     "start_time": "2025-04-30T08:45:53.623681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "N = params()[\"total\"]  # this is number of parameters, N\n",
    "D = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "a100_bfloat16_promised_flops = 312e12  # 312 TFLOPS\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "flops_throughput = a100_bfloat16_promised_flops * 8 * assumed_mfu  # assume an 8XA100 node at 30% utilization\n",
    "flops_needed = 6 * N * D  # 6ND\n",
    "time_needed_over_all_tokens_in_seconds = flops_needed / flops_throughput  # in seconds\n",
    "print(f\"time needed to train the model: {time_needed_over_all_tokens_in_seconds/3600/24:.2f} days\")"
   ],
   "id": "d94a3ca40c3f5aa9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to train the model: 3.46 days\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
